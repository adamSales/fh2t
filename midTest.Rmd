---
title: "Mid-Test Effects"
output: 
    bookdown::pdf_document2: default
---

```{r prelim, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE,error=FALSE,cache=FALSE)

library(tidyverse)
library(readxl)
library(mosaic)
library(lme4)
library(RItools)
library(texreg)
library(knitr)
library(kableExtra)
library(estimatr)
```

```{r readData}

Scale <- function(x) (x-mean(x,na.rm=TRUE))/sd(x,na.rm=TRUE)

dat <- read_csv('data/IES_assessment_final_2021_06_03_N=4284 - Sheet1.csv')%>% 
    rename(condition=condition_assignment)%>%
    filter(KEEP==1&condition%in%c('Delay','Instant'))%>%
    mutate(midMis=is.na(mid.total_math_score),
           preMis=is.na(pre.total_math_score),
           race=student_raceEthnicityFed%>%
               factor()%>%
               fct_lump_min(200)%>%
               fct_recode(`Hispanic/Latino`="1",Asian="3",White="6")%>%
               fct_relevel('White'),
           pretest = Scale(pre.total_math_score),
           ScaleScore = Scale(ScaleScore),
           inst = as.numeric(condition=='Instant'),
           pretestNeg = pretest-1,
           pretestPos = pretest+1,
           )

 ### delete all observations from classrooms in which we don't have students in both treatment conditions with both pre and mid test scores -->
 smallDat <- dat%>% 
     filter(!preMis,!midMis)%>%
     group_by(initial_teacher_class)%>%
     mutate(nInst=sum(condition=='Instant'),nDel=sum(condition=='Delay'))%>%
     filter(nInst>0,nDel>0)%>%
     ungroup()%>%droplevels()

```


# Introduction

This document estimates the effect of immediate versus delayed feedback on the mid-test (total math score) in the FH2T RCT.
It is based on the `r nrow(dat)` subjects initially randomized between the two conditions, `r sum(dat$condition=='Instant')` randomized to the Instant condition, and `r sum(dat$condition=='Delay')` randomized to the delayed feedback condition.
Randomization was blocked within the `r n_distinct(dat$initial_teacher_class)` classrooms.

The following section discusses attrition--students who did not take the mid-assessment.

The next section estimates treatment effects.

# Attrition

When randomized subjects do not have outcome information in an RCT, effect estimates may be biased.
Randomization creates treatment groups that, in expectation, are identical in measured and unmeasured characteristics, referred to as "covariate balance."
However, attrition is not controlled by the researcher, nor is it random; in particular, if different types of subjects attrit in different treatment groups, the remaining subjects will not necessarily be comparable across groups.
That said, if the overall level of attrition is low, or if the level
of attrition is similar between the two treatment groups, and if important covariates remain balanced
between treatment groups after excluding attritors, the bias might be
negligible.

The What Works Clearinghouse publishes [standards for attrition bias][1]
based on the level of overall attrition as well as differential
attrition--the difference in attrition levels between treatment
groups. 

```{r numAtt,results='hide'}
numAtt1 <- dat%>%
    mutate(itc=as.factor(initial_teacher_class))%>%
    filter(!is.na(mid.total_math_score))%>%
    group_by(itc,.drop=FALSE)%>%
    summarize(n=n(),bothDrop=n()==0,oneDrop=!bothDrop&(sum(condition=='Instant')==0|sum(condition=='Delay')==0))%>%
    ungroup()%>%
    summarize(both=sum(bothDrop),one=sum(oneDrop),stud=sum(n[oneDrop]))

numAtt2 <- dat%>%
    mutate(itc=as.factor(initial_teacher_class))%>%
    filter(!is.na(mid.total_math_score),!is.na(pre.total_math_score))%>%
    group_by(itc,.drop=FALSE)%>%
    summarize(n=n(),bothDrop=n()==0,oneDrop=!bothDrop&(sum(condition=='Instant')==0|sum(condition=='Delay')==0))%>%
    ungroup()%>%
    summarize(both=sum(bothDrop),one=sum(oneDrop),stud=sum(n[oneDrop]))
```

In practice, we will omit the `r prettyNum(sum(dat$midMis),big.mark=',')` students without mid-test scores (and in some analyses, also the additional `r sum(dat$preMis&!dat$midMis)` students without pre-test scores). 
In `r numAtt1$both` classrooms, no students took the mid-test, and in `r numAtt2$both-numAtt1$both` additional classrooms, no student took both the pre- and the mid-test.
In `r numAtt1$one` classrooms, there were no students with pre-test scores in one of the two treatment groups; these classrooms were dropped from all analyses other than Model 1, below. Dropping these entire classrooms enhances the validity of the analyses, but reduces the sample size by `r numAtt1$stud` students.

## Attrition Rates

The overall attrition for the mid-test was `r round(mean(dat$midMis)*100)`%--`r sum(dat$midMis)` students out of `r nrow(dat)` did not complete the mid-test. 

```{r attritionTable1,results='asis'}
dat%>%mutate(midMis=ifelse(midMis,'Attrit','Took Mid-Test'))%>%
tally(midMis~condition,data=.,format="percent")%>%
    kbl(booktabs=TRUE,caption="Attrition by treatment group.",digits=1)
```

Table \@ref(tab:attritionTable1) gives attrition by treatment
group. 
The differential attrition was `r round((mean(dat$midMis[dat$condition=='Instant'])-mean(dat$midMis[dat$condition!='Instant']))*100,2)`%.
Taken together, the (high) overall attrition with the (low) differential attrition means that this study meets the conservative WWC standards. 

An important subgroup is composed of the `r sum(!dat$preMis)`=`r round((1-mean(dat$preMis))*100)`% of students who have pre-test scores. 
Among this subgroup, attrition is much lower: ``r round(mean(dat$midMis[!dat$preMis])*100)`%. 
The differential attrition is also acceptable: 
`r with(dat[!dat$preMis,],round((mean(midMis[condition=='Instant'])-mean(midMis[condition!='Instant']))*100,2))`%.

## Attrition and Covariates

### Who Attritted?

    
```{r whoAttrit,fig.width=6.5,fig.height=4,fig.cap="Standardized differences of covariate means between students who took the mid-test and those who didn't"}
balMid <- xBalance(!midMis~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,data=dat,
                   report='all',strata=list(classroom=~initial_teacher_class))
balMidSmall <- xBalance(!midMis~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,data=filter(dat,!preMis),
                   report='all',strata=list(classroom=~initial_teacher_class))
plot(balMid,ggplot=TRUE)+theme(legend.position='none')+annotate("text",x=c(-.1,.1),y=c(18,18),label=c('Attritted','Took\ntest'))+xlim(-.2,.41)+
    annotate("segment", x = -.15, xend = -.2, y = 10, yend = 10, colour = "red", size=1, arrow=arrow())

```

Who attritted? Figure \@ref(fig:whoAttrit) shows standardized differences of covariates between students who attritted and those who took the mid-test. 
Students who took the test had higher pretest and 5th-grade scale scores, were more likely to be gifted, and more likely to be Asian that students who attritted. 
Students who attritted were much less likely to have taken the pre-test. 

### Covariate Balance among Test-Takers

```{r balAttrit,fig.width=6.5,fig.height=4,fig.cap="Standardized differences of covariate means between Instant and Delayed-feedback students among non-attritors"}

balAtt <- xBalance(I(condition=="Instant")~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,
                   data=dat[!dat$preMis&!dat$midMis,],
                   report='all',strata=list(classroom=~initial_teacher_class))

plot(balAtt,ggplot=TRUE)+theme(legend.position='none')+annotate("text",x=c(-.05,.05),y=c(18,18),label=c('Delayed','Instant'))
```

Are instant- and delayed-feedback non-attritors comparable? 
Figure \@ref(fig:balAttrit) compares the two treatment groups among those who took the mid-test. 
A p-value testing overall balance was $p=$`r round(balAtt$overall[1,'p.value'],3)`, meaning that the groups were more balanced than `r round(balAtt$overall[1,'p.value']*100)`% of randomized experiments.

The only notable difference was that delayed feedback students who took the mid-test tend to be more likely to be ESOL than those in the instant-feedback condition.

These results suggest that confounding bias due to attrition is unlikely to be a major concern.
Nevertheless, we adjust for pre-treatment covariates in one of the models below.



# Treatment Effects

```{r boxplot,fig.width=4,fig.height=4,fig.cap='Boxplots of Mid-test scores for the two treatment conditions, with jittered scores'}
gf_boxplot(mid.total_math_score~condition,data=dat,outlier.shape=NA)+geom_jitter()+labs(y='Mid. Total Math Score',x=NULL)+
    scale_y_continuous(breaks=seq(0,10,2),minor_breaks=0:10)
```

Figure \@ref(fig:boxplot) gives boxplots of the total scores for the two treatment conditions, with individual scores plotted as jittered points. 
The median score for Instant-feedback students is one point higher than the median for delayed-feedback students, but there is wide variation in both groups.

```{r byScore,fig.width=6,fig.height=3,fig.cap='The percent of each treatment group achieving each score on the mid-test'}
dat%>%group_by(condition)%>%mutate(nt=sum(!is.na(mid.total_math_score)))%>%group_by(condition,mid.total_math_score)%>%summarize(percent=n()/nt)%>%ggplot(aes(mid.total_math_score,percent,color=condition,group=condition))+geom_point()+geom_line()+scale_x_continuous(breaks=0:10)+scale_y_continuous(labels=scales::percent,limits=c(0,.15))
```

Figure \@ref(fig:byScore) shows the percent of students in each treatment group who achieved each possible score on the mid-test. 
A slightly higher percentage of delayed-feedback students scored 3, 4, or 5, and a slightly higher percentage of instant-feedback students scored 6 or 9.


```{r prePostScatter,fig.width=6.4,fig.height=4,fig.cap='Scatterplot of pre- and mid-test scores, with jitter to avoid overplotting and separate OLS fits by treatment group.'}
gf_jitter(mid.total_math_score~pre.total_math_score,color=~condition,data=dat,size=.5)+geom_smooth(method='lm')+
    scale_x_continuous('Pretest',breaks=seq(2,10,2),minor_breaks=seq(1,9,2))+
    scale_y_continuous('Midtest',breaks=seq(2,10,2),minor_breaks=seq(1,9,2))
```
    
Figure \@ref(fig:prePostScatter) gives a scatterplot of pre-test vs post-test scores with separate OLS fits in each treatment group. The best-fit lines are almost identical, but not quite--ignoring statistical error, it appears that students with very high pre-test scores did better on the mid-test if they were in the Instant condition, while students with very low pre-test scores did better on the mid-test if they were in the Delay condition. 
We can test this with an OLS model below.
	
Table \@ref(tab:ols) give the results of OLS models estimating the effect of assignment to the instant- versus delayed-feedback condition on mid-tests.
Model 1 is just a comparison of the mean test scores.
Models 2-4 also include fixed-effects for classroom (the randomization blocks). 
These estimate a weighted-average treatment effect, where the weights are chosen to maximize precision.
Models 3-4 adjust for covariates, and include only students with pretest scores. 
All standard errors are heteroskedasticity-consistent, estimated using the `estimatr` package in `R`.


```{r dataForModels,include=FALSE,cache=TRUE}
## dat <- dat%>%
##     group_by(initial_teacher_class)%>%
##     mutate(
##         itc1=ifelse(sum(!midMis&condition=='Instant')>0&sum(!midMis&condition!='Instant')>0,initial_teacher_class,"new"),
##         itc2=ifelse(sum(!midMis&!preMis&condition=='Instant')>0&sum(!midMis&!preMis&condition!='Instant')>0,initial_teacher_class,"new")
##     )%>%
##     ungroup()

covs <- smallDat%>%select(pretest,ScaleScore,EIP,ESOL,IEP,FEMALE,GIFTED,race)%>%
    mutate(across(c(EIP,ESOL,IEP,FEMALE,GIFTED),factor)) 

covsImp <- missForest::missForest(as.data.frame(covs),variablewise=TRUE)

covsImp$OOB

for(nn in names(covs)){
    if(mean(is.finite(smallDat[[nn]]))<1){
        smallDat[[paste0(nn,'MISS')]] <- is.na(smallDat[[nn]])
        smallDat[[nn]] <- covsImp$ximp[[nn]]
    }
}       

```


```{r models,results='asis'}
mod1 <- lm_robust(mid.total_math_score~condition,data=dat)
mod2 <- lm_robust(mid.total_math_score~condition,data=dat,fixed_effects=~initial_teacher_class)
mod4 <- lm_robust(mid.total_math_score~condition+pretest,data=smallDat,fixed_effects=~initial_teacher_class)
mod6 <- lm_robust(mid.total_math_score~condition+pretest+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=smallDat,fixed_effects=~initial_teacher_class)
mod7 <- lm_robust(mid.total_math_score~condition+pretest+condition:pretest+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=smallDat,fixed_effects=~initial_teacher_class)
mod7neg<- lm_robust(mid.total_math_score~condition+pretestNeg+condition:pretestNeg+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=smallDat,fixed_effects=~initial_teacher_class)
mod7pos<- lm_robust(mid.total_math_score~condition+pretestPos+condition:pretestPos+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=smallDat,fixed_effects=~initial_teacher_class)

texreg(list(mod1,mod2,mod4,mod6,mod7),caption="OLS models estimating the effect of assignment to the instant feedback condition versus the delayed feedback condition. All models except 11 include fixed-effects for classroom; models 3 \\& 4 only include students with pretest scores. Confidence intervals are in brackets under coefficient estimates.",label='tab:ols')
```

All of the models estimated a positive effect for instant- versus delayed-feedback, but none of them estimated significant effects--all models agree that the instant versus delayed feedback affects average scores by at most half of a point.

Model 7 estimated a positive interaction between the effect of being assigned to the Instant condition and pretest scores--i.e. that the effect becomes larger (more positive) as pretest scores increase--however, the data are also consistent with a 0 or slightly negative interaction 
(p=`r round(mod7$p.value['conditionInstant:pretest'],3)`).
The model predicts an effect of `r round(mod7neg$coefficients['conditionInstant'],3)`, (CI: [`r round(mod7neg$conf.low[1],3)`,`r round(mod7neg$conf.high[1],3)`]) for subjects with pretest scores 1 standard deviation above the mean and 
`r round(mod7pos$coefficients['conditionInstant'],3)`, (CI: [`r round(mod7pos$conf.low[1],3)`,`r round(mod7pos$conf.high[1],3)`]) for subjects with pretest scores 1 standard deviation below the mean.


# Treatment Effect by Item

Using `r nrow(smallDat)` observations where in each stratum there's at
least one student in each treatment condition with both pre- and
mid-test scores.

```{itemData,include=FALSE}
datLongPre <- smallDat%>%
    select(student_number,starts_with("correct.01_"))%>%
    pivot_longer(cols=-student_number,names_to='prob',values_to='pre',names_prefix='correct.01_')%>%
    mutate(prob=substr(prob,1,2))

datLongMid <- smallDat%>%
    select(student_number,starts_with("correct.06_"))%>%
    pivot_longer(cols=-student_number,names_to='prob',values_to='mid',names_prefix='correct.06_')%>%
    mutate(prob=substr(prob,1,2))

datLong <-
    smallDat%>%
    select(student_number,initial_teacher_class,condition,ScaleScore,EIP,ESOL,IEP,FEMALE,GIFTED,race,ScaleScoreMISS,ESOLMISS,raceMISS)%>%
    right_join(datLongPre,by='student_number')%>%
    full_join(datLongMid,by=c('student_number','prob'))
```

```{r gainByProb,fig.width=6.4,fig.height=4,fig.cap='Proportion getting each mid-test item correct minus the proportion getting the analogous pre-test item correct, for both treatment groups'}

datLong%>%
    group_by(prob,condition)%>%
    summarize(gain=mean(mid)-mean(pre))%>%
    ggplot(aes(condition,gain,fill=condition))+
    geom_hline(yintercept=0)+
    geom_col()+
    facet_wrap(~prob)
ggsave('gainByProb.jpg')
```


```{r byProbModels,include=FALSE,cache=TRUE}
mod0 <- glmer(mid~condition+pre+(1|student_number/initial_teacher_class)+(1|prob),data=datLong,family=binomial)
mod1 <- update(mod0,.~.+ScaleScore)
mod2 <- update(mod1,.~.-(1|prob)+(pre|prob))
an12 <- anova(mod1,mod2)
mod3 <- update(mod1,.~.-(pre|prob)+(pre+condition|prob))
an23 <- anova(mod1,mod2,mod3)
```

```{r getEffs, include=FALSE,cache=TRUE}
re3 <- ranef(mod3,condVar=TRUE)
effByProb <- fixef(mod3)['conditionInstant']+re3$prob$condition
pv <- attr(re3$prob,'postVar')
seByProb <- sqrt(summary(mod3)$coef['conditionInstant',2]^2+pv[[2]][3,3,])
```

```{r effByProbPlot,fig.width=6.4,fig.height=4,fig.cap='Effect of being assigned to "Instant" condition, by each problem, with approximate 95% confidence intervals.'}

data.frame(item=rownames(re3$prob),effect=effByProb,seEff=seByProb)%>%
    mutate(signif=ifelse(effect-2*seEff>0,'Positive',ifelse(effect+2*seEff<0,'Negative','Undetermined')))%>%
    ggplot(aes(item, effect,ymin=effect-2*seEff,ymax=effect+2*seEff))+
    geom_point()+
    geom_errorbar(width=0)+geom_hline(yintercept=0)+
    labs(x='Item',y='Treatment Effect (Logit Scale)',color='Year')
ggsave('effectByProblem.jpg')
```

[1]:<https://ies.ed.gov/ncee/wwc/Document/243>
