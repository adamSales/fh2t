---
title: "Mid-Test Effects"
output:
  bookdown::html_document2: default
---

```{r prelim, include=FALSE}
knitr::opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE,error=FALSE,cache=FALSE)

library(tidyverse)
library(readxl)
library(mosaic)
library(lme4)
library(RItools)
library(texreg)
library(knitr)
library(kableExtra)
library(estimatr)
library(modelsummary)
```

```{r readData}

Scale <- function(x) (x-mean(x,na.rm=TRUE))/sd(x,na.rm=TRUE)

dat <- read_csv('../data/IES_assessment_final_2021_06_03_N=4284 - Sheet1.csv')%>% 
    rename(condition=condition_assignment)%>%
    filter(KEEP==1&condition%in%c('Delay','Instant'))%>%
    mutate(midMis=is.na(mid.total_math_score),
           preMis=is.na(pre.total_math_score),
           postMis=is.na(post.total_math_score),
           race=student_raceEthnicityFed%>%
               factor()%>%
               fct_lump_min(200)%>%
               fct_recode(`Hispanic/Latino`="1",Asian="3",White="6")%>%
               fct_relevel('White'),
           pretest = pre.total_math_score-round(mean(pre.total_math_score,na.rm=TRUE)),#Scale(pre.total_math_score),
           ScaleScore = Scale(ScaleScore),
           inst = as.numeric(condition=='Instant'),
           pretestPos = pre.total_math_score-round(mean(pre.total_math_score,na.rm=TRUE)+sd(pre.total_math_score,na.rm=TRUE)),
           pretestNeg = pre.total_math_score-round(mean(pre.total_math_score,na.rm=TRUE)-sd(pre.total_math_score,na.rm=TRUE)),
           `Race/Ethnicity`=factor(student_raceEthnicityFed)%>%
               fct_recode(
                   `Hispanic/Latino`="1", 
                   `American Indian/Alaska Native`="2",
                   Asian="3", 
                   `Black/African American`="4",
                   `Native Hawaiian or Other Pacific islander`="5",
                   White="6", 
                   `Two or more races`="7")%>%
               fct_explicit_na("Unknown"),
           Gender=factor(ifelse(FEMALE==1,'Female','Male'))%>%fct_explicit_na("Unknown"))


### delete all observations from classrooms in which we don't have students in both treatment conditions with both pre and mid test scores -->
 smallDat <- dat%>% 
     filter(!preMis,!postMis)%>%
     group_by(initial_teacher_class)%>%
     mutate(nInst=sum(condition=='Instant'),nDel=sum(condition=='Delay'))%>%
     filter(nInst>0,nDel>0)%>%
     ungroup()%>%droplevels()


finalDat <- dat%>%
    filter(!preMis,!postMis)%>%
    mutate(initial_teacher_class=fct_explicit_na(initial_teacher_class))
    
finalDat%>%select(Gender,`Race/Ethnicity`)%>%datasummary_skim(type="categorical",output="table1.docx")
finalDat%>%select(Gender,`Race/Ethnicity`)%>%datasummary_skim(type="categorical")
```



# Introduction

This document estimates the effect of immediate versus delayed feedback on the post-test (total math score) in the FH2T RCT.
It is based on the `r nrow(dat)` subjects initially randomized between the two conditions, `r sum(dat$condition=='Instant')` randomized to the Instant condition, and `r sum(dat$condition=='Delay')` randomized to the delayed feedback condition.
Randomization was blocked within the `r n_distinct(dat$initial_teacher_class)` classrooms.

The following section discusses attrition--students who did not take the post-assessment.

The next section estimates treatment effects.



# Attrition

When randomized subjects do not have outcome information in an RCT, effect estimates may be biased.
Randomization creates treatment groups that, in expectation, are identical in measured and unmeasured characteristics, referred to as "covariate balance."
However, attrition is not controlled by the researcher, nor is it random; in particular, if different types of subjects attrit in different treatment groups, the remaining subjects will not necessarily be comparable across groups.
That said, if the overall level of attrition is low, or if the level
of attrition is similar between the two treatment groups, and if important covariates remain balanced
between treatment groups after excluding attritors, the bias might be
negligible.

The What Works Clearinghouse publishes [standards for attrition bias][1]
based on the level of overall attrition as well as differential
attrition--the difference in attrition levels between treatment
groups. 

```{r numAtt,results='hide'}
numAtt1 <- dat%>%
    mutate(itc=as.factor(initial_teacher_class))%>%
    filter(!is.na(post.total_math_score))%>%
    group_by(itc,.drop=FALSE)%>%
    summarize(n=n(),bothDrop=n()==0,oneDrop=!bothDrop&(sum(condition=='Instant')==0|sum(condition=='Delay')==0))%>%
    ungroup()%>%
    summarize(both=sum(bothDrop),one=sum(oneDrop),stud=sum(n[oneDrop]))

numAtt2 <- dat%>%
    mutate(itc=as.factor(initial_teacher_class))%>%
    filter(!is.na(post.total_math_score),!is.na(pre.total_math_score))%>%
    group_by(itc,.drop=FALSE)%>%
    summarize(n=n(),bothDrop=n()==0,oneDrop=!bothDrop&(sum(condition=='Instant')==0|sum(condition=='Delay')==0))%>%
    ungroup()%>%
    summarize(both=sum(bothDrop),one=sum(oneDrop),stud=sum(n[oneDrop]))
```

In practice, we will omit the `r prettyNum(sum(dat$postMis),big.mark=',')` students without post-test scores (and in some analyses, also the additional `r sum(dat$preMis&!dat$postMis)` students without pre-test scores). 
In `r numAtt1$both` classrooms, no students took the post-test, and in `r numAtt2$both-numAtt1$both` additional classrooms, no student took both the pre- and the post-test.
In `r numAtt1$one` classrooms, there were no students with pre-test scores in one of the two treatment groups; these classrooms were dropped from all analyses other than Model 1, below. Dropping these entire classrooms enhances the validity of the analyses, but reduces the sample size by `r numAtt1$stud` students.

## Attrition Rates

The overall attrition for the post-test was `r round(mean(dat$postMis)*100)`%--`r sum(dat$postMis)` students out of `r nrow(dat)` did not complete the post-test. 

```{r attritionTable1,results='asis'}
dat%>%mutate(postMis=ifelse(postMis,'Attrit','Took Post-Test'))%>%
tally(postMis~condition,data=.,format="percent")%>%
    kbl(booktabs=TRUE,caption="Attrition by treatment group.",digits=1)
```

Table \@ref(tab:attritionTable1) gives attrition by treatment
group. 
The differential attrition was `r round((mean(dat$postMis[dat$condition=='Instant'])-mean(dat$postMis[dat$condition!='Instant']))*100,2)`%.
Taken together, the (high) overall attrition with the (low) differential attrition means that this study meets the conservative WWC standards. 

An important subgroup is composed of the `r sum(!dat$preMis)`=`r round((1-mean(dat$preMis))*100)`% of students who have pre-test scores. 
Among this subgroup, `r sum(!dat$preMis&!dat$postMis)` also took the
post-test, so attrition is much lower: `r round(mean(dat$postMis[!dat$preMis])*100)`%. 
The differential attrition is also acceptable: 
`r with(dat[!dat$preMis,],round((mean(postMis[condition=='Instant'])-mean(postMis[condition!='Instant']))*100,2))`%.

## Attrition and Covariates

### Who Attritted?

    
```{r whoAttrit,fig.width=6.5,fig.height=4,fig.cap="Standardized differences of covariate means between students who took the post-test and those who didn't"}
balPost <- xBalance(!postMis~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,data=dat,
                   report='all',strata=list(classroom=~initial_teacher_class))
balPostSmall <- xBalance(!postMis~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,data=filter(dat,!preMis),
                   report='all',strata=list(classroom=~initial_teacher_class))
plot(balPost,ggplot=TRUE)+theme(legend.position='none')+annotate("text",x=c(-.1,.1),y=c(18,18),label=c('Attritted','Took\ntest'))+xlim(-.2,.41)+
    annotate("segment", x = -.15, xend = -.2, y = 10, yend = 10, colour = "red", size=1, arrow=arrow())

```

Who attritted? Figure \@ref(fig:whoAttrit) shows standardized differences of covariates between students who attritted and those who took the post-test. 
Students who took the test had higher pretest and 5th-grade scale scores, were more likely to be gifted, and more likely to be Asian that students who attritted. 
Students who attritted were much less likely to have taken the pre-test. 

### Covariate Balance among Test-Takers

```{r balAttrit,fig.width=6.5,fig.height=4,fig.cap="Standardized differences of covariate means between Instant and Delayed-feedback students among non-attritors"}

balAtt <- xBalance(I(condition=="Instant")~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+race,
                   data=finalDat,
                   report='all',strata=list(classroom=~initial_teacher_class))

balAtt0 <- xBalance(I(condition=="Instant")~ScaleScore+pre.total_math_score+EIP+ESOL+IEP+FEMALE+GIFTED+student_hispanicEthnicity+race,
                   data=dat[!dat$postMis,],
                   report='all',strata=list(classroom=~initial_teacher_class))

plot(balAtt,ggplot=TRUE)+theme(legend.position='none')+annotate("text",x=c(-.05,.05),y=c(18,18),label=c('Delayed','Instant'))
```

```{r covariateImbalance,results='asis'}
balAtt$results[,,1]%>%
    as.data.frame()%>%
    rownames_to_column('Covariate')%>%
    filter(abs(std.diff)>0.05)%>%
    arrange(-abs(std.diff))%>%
    mutate(z=paste0(round(z,2),ifelse(p<0.001,'***',ifelse(p<0.01,'**',ifelse(p<0.05,'*',ifelse(p<0.1,'.',''))))))%>%
    transmute(Covariate,"Delayed"="Control","Instant"="Treatment",std.diff,Zscore=z)%>%
    kable(digits=3)
```

Are instant- and delayed-feedback non-attritors comparable? 
Figure \@ref(fig:balAttrit) compares the two treatment groups among those who took the post-test. 
A p-value testing overall balance was $p=$`r round(balAtt$overall[1,'p.value'],3)`, meaning that the groups were more balanced than `r round(balAtt$overall[1,'p.value']*100)`% of randomized experiments.

The only notable difference was that delayed feedback students who took the post-test tend to be more likely to be ESOL than those in the instant-feedback condition.

These results suggest that confounding bias due to attrition is unlikely to be a major concern.
Nevertheless, we adjust for pre-treatment covariates in one of the models below.

# Growth

```{r growth}
t.test(smallDat$post.total_math_score-smallDat$pre.total_math_score)
```

# Treatment Effects

```{r boxplot,fig.width=4,fig.height=4,fig.cap='Boxplots of Post-test scores for the two treatment conditions, with jittered scores'}
gf_boxplot(post.total_math_score~condition,data=finalDat,outlier.shape=NA)+geom_jitter()+labs(y='Post. Total Math Score',x=NULL)+
    scale_y_continuous(breaks=seq(0,10,2),minor_breaks=0:10)
```

Figure \@ref(fig:boxplot) gives boxplots of the total scores for the two treatment conditions, with individual scores plotted as jittered points. 
The median score for Instant-feedback students is one point higher than the median for delayed-feedback students, but there is wide variation in both groups.

```{r byScore,fig.width=6,fig.height=3,fig.cap='The percent of each treatment group achieving each score on the post-test'}
finalDat%>%group_by(condition)%>%mutate(nt=sum(!is.na(post.total_math_score)))%>%group_by(condition,post.total_math_score)%>%summarize(percent=n()/nt)%>%ggplot(aes(post.total_math_score,percent,color=condition,group=condition))+geom_point()+geom_line()+scale_x_continuous(breaks=0:10)+scale_y_continuous(labels=scales::percent,limits=c(0,.15))
```

Figure \@ref(fig:byScore) shows the percent of students in each treatment group who achieved each possible score on the post-test. 
A slightly higher percentage of delayed-feedback students scored 3, 4, or 5, and a slightly higher percentage of instant-feedback students scored 6 or 9.


```{r prePostScatter,fig.width=6.4,fig.height=4,fig.cap='Scatterplot of pre- and post-test scores, with jitter to avoid overplotting and separate OLS fits by treatment group.'}
gf_jitter(post.total_math_score~pre.total_math_score,color=~condition,data=finalDat,size=.5)+geom_smooth(method='lm')+
    scale_x_continuous('Pretest',breaks=seq(2,10,2),minor_breaks=seq(1,9,2))+
    scale_y_continuous('Posttest',breaks=seq(2,10,2),minor_breaks=seq(1,9,2))
```
    
Figure \@ref(fig:prePostScatter) gives a scatterplot of pre-test vs post-test scores with separate OLS fits in each treatment group. The best-fit lines are almost identical, but not quite--ignoring statistical error, it appears that students with very high pre-test scores did better on the post-test if they were in the Instant condition, while students with very low pre-test scores did better on the post-test if they were in the Delay condition. 
We can test this with an OLS model below.
	
Table \@ref(tab:ols) give the results of OLS models estimating the effect of assignment to the instant- versus delayed-feedback condition on post-tests.
Model 1 is just a comparison of the mean test scores.
Models 2-4 also include fixed-effects for classroom (the randomization blocks). 
These estimate a weighted-average treatment effect, where the weights are chosen to maximize precision.
Models 3-4 adjust for covariates, and include only students with pretest scores. 
All standard errors are heteroskedasticity-consistent, estimated using the `estimatr` package in `R`.


```{r impute,include=FALSE,cache=TRUE}
## dat <- dat%>%
##     group_by(initial_teacher_class)%>%
##     mutate(
##         itc1=ifelse(sum(!postMis&condition=='Instant')>0&sum(!postMis&condition!='Instant')>0,initial_teacher_class,"new"),
##         itc2=ifelse(sum(!postMis&!preMis&condition=='Instant')>0&sum(!postMis&!preMis&condition!='Instant')>0,initial_teacher_class,"new")
##     )%>%
##     ungroup()

covs <- finalDat%>%select(pretest,ScaleScore,EIP,ESOL,IEP,FEMALE,GIFTED,race)%>%
    mutate(across(c(EIP,ESOL,IEP,FEMALE,GIFTED),factor)) 

covsImp <- missForest::missForest(as.data.frame(covs),variablewise=TRUE)
```

```{r dataForModels,include=FALSE}
#covsImp$OOB
for(nn in names(covs)){
    if(mean(is.finite(finalDat[[nn]]))<1){
        finalDat[[paste0(nn,'MISS')]] <- is.na(finalDat[[nn]])
        finalDat[[nn]] <- covsImp$ximp[[nn]]
    }
}       

```





```{r models,results='asis'}
mod0 <- lm_robust(post.total_math_score~condition,data=finalDat)
mod1 <- lm_robust(post.total_math_score~condition,data=finalDat,fixed_effects=~initial_teacher_class)
mod2 <- lm_robust(post.total_math_score~condition+pretest+ScaleScore+ESOL+IEP+race,data=finalDat,fixed_effects=~initial_teacher_class)
mod3 <- lm_robust(post.total_math_score~condition+pretest+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=finalDat,fixed_effects=~initial_teacher_class)
mod3lm <- lm(post.total_math_score~condition+pretest+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS+initial_teacher_class,data=finalDat)
mod4 <- lm_robust(post.total_math_score~condition*pretest+ScaleScore+ESOL+IEP+race,data=finalDat,fixed_effects=~initial_teacher_class)
mod5 <- lm_robust(post.total_math_score~condition+pretest+condition:pretest+ScaleScore+EIP+ESOL+IEP+FEMALE+GIFTED+race+ScaleScoreMISS+ESOLMISS+raceMISS,data=finalDat,fixed_effects=~initial_teacher_class)
mod4lm <- update(mod3lm,.~.+pretest:condition) 
mod4neg<- lm_robust(post.total_math_score~condition+pretestNeg+condition:pretestNeg+ScaleScore+ESOL+IEP+race,data=finalDat,fixed_effects=~initial_teacher_class)
mod4pos<- lm_robust(post.total_math_score~condition+pretestPos+condition:pretestPos+ScaleScore+ESOL+IEP+race,data=finalDat,fixed_effects=~initial_teacher_class)

htmlreg(list(mod1,mod2,mod3,mod4,mod5),caption="OLS models estimating the effect of assignment to the instant feedback condition versus the delayed feedback condition. Confidence intervals are in brackets under coefficient estimates.",label='tab:ols',stars=c(0.1,0.05,0.01,0.001))
```

```{r confint}
map(list(mod1,mod2,mod3), confint,parm='conditionInstant')
map(list(mod4,mod5),confint,parm=c('conditionInstant','conditionInstant:pretest'))
```

```{r funcs}
rnd <- function(x) sprintf("%1.3f",x)
ci <- function(x) paste0('[',rnd(x[1]),',',rnd(x[2]),']')
ci2 <- function(x){
    est <- mean(x)
    moe <- (x[2]-x[1])/2
    paste(rnd(est),"$\\pm$",rnd(moe))
}
```
 Regression model results are included in Table 2. Model 2, including
 adjustments for slightly imbalanced covariates, estimates an effect
 of being assigned to immediate feedback of `r ci2(confint(mod2,'conditionInstant'))` points with 95%
 confidence--results consistent with a small average preference for
 either delayed or immediate feedback (B=`r rnd(mod2$coef['conditionInstant'])`, p=`r rnd(mod2$p.value['conditionInstant'])`). Models 1 and 3, adjusting for fewer or more covariates, respectively, give similar estimates.
 
The interaction between condition and pretest scores was tested in Models 4 and 5 of Table 2, and displayed in Figure 4.
According to Model 4, each additional point on the pretest is associated with a change `r ci2(confint(mod4,'conditionInstant:pretest'))` in the effect of immediate feedback (p=`r rnd(mod4$p.value['conditionInstant:pretest'])`). 
 For students scoring 1 standard deviation below or above the mean on the pretest, the estimated effect of immidiate feedback is 
 `r ci2(confint(mod4neg,'conditionInstant'))` or `r ci2(confint(mod4pos,'conditionInstant'))`, respectively.
 Model 5, adjusting for more covariates, gives similar estimates. 

```{r regTableForWord,include=FALSE}
if(require(modelsummary))
    modelsummary(list(mod1,mod2,mod3,mod4,mod5),title="OLS models estimating the effect of assignment to the instant feedback condition versus the delayed feedback condition. All models except 11 include fixed-effects for classroom; models 3 \\& 4 only include students with pretest scores. Confidence intervals are in brackets under coefficient estimates.",output="regressionTable.docx",stars=TRUE)
```


[1]:<https://ies.ed.gov/ncee/wwc/Document/243>
