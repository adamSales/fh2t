---
title: "Appendix [?]: Attrition Analysis"
output: word_document
---

```{r init, include=FALSE}
library(knitr)
opts_chunk$set(echo=FALSE,message=FALSE,warning=FALSE,error=FALSE,fig.width=9,fig.height=9,
  cache=TRUE)
options(knitr.kable.NA='-')
```

```{r loadCode, include=FALSE}
library(tidyverse)
library(RItools)
source("../code/functions.r")
Scale <- function(x) (x-mean(x,na.rm=TRUE))/sd(x,na.rm=TRUE)
condNames=c(FH2T='FH2T',Dragon='DragonBox',ASSISTments='Immediate Feedback',BAU='Active Control')
```

```{r loadData}
## randomized data
dat0 <- read_csv("../data/DATA20220202_4092.csv",na = c("", "NA","#NULL!"))%>%
  filter(
    !endsWith(rdm_condition,'Resource'))%>%
  mutate(
    ScaleScore7 = Scale(Scale.Score7),
    hasPretest=is.finite(pre.total_math_score),
    hasPosttest=is.finite(post.total_math_score),
    Z =rdm_condition,
    race=raceEthnicityFed%>%
               factor()%>%
               fct_lump(n=2)%>%
               fct_recode(Asian="3",White="6")%>%
            fct_relevel("White"),
      ScaleScore5 = Scale(`Scale.Score5`),
      accelerated=grepl('Accelerated',courseName),
      pretest = pre.total_math_score-round(mean(pre.total_math_score,na.rm=TRUE))
  )%>%
  filter(!is.na(Z))%>%
  mutate(
    teach=ifelse(is.na(TeaIDPre),TeaIDEnd,TeaIDPre),
    class=ifelse(is.na(ClaIDPre),ClaIDEnd,ClaIDPre))

noPre=dat0%>%
  group_by(SchIDPre)%>%
  summarize(pretest=mean(hasPretest))%>%
  filter(pretest<0.01)%>%
  pull(SchIDPre)
### Which teachers?
noPreTch=unique(dat0$TeaIDPre[dat0$SchIDPre==noPre])

dat0$dropsch1a=dat0$teach%in%noPreTch

diffStuds=dat0$StuID[dat0$DROPSCH1==1 & !dat0$dropsch1a]
ndiff=length(diffStuds)

dat0$Z <- relevel(factor(dat0$Z),ref='BAU')

covNames <-
    c("pretest","race","FEMALE","EIP","IEP","GIFTED","virtual","accelerated") ## more?
```


```{r analysisSample}
dat0 <- dat0%>%filter(!dropsch1a)


### analysis sample
dat <- dat0%>%
    filter(DROPSCH2==0,hasPretest,hasPosttest)
```


[[NOTE: THE FOLLOWING IS PARAGRAPH IS A VERBATIM COPY OF A PARAGRAPH IN THE MAIN MANUSCRIPT. IF WE LEAVE IT IN THERE, WE SHOULD REMOVE IT HERE]]

Our approach to causal inference under attrition, following WWC (2020) and others, is to attempt to estimate treatment effects for the subset of students in our analysis sample, i.e. those who did not attrit. Under this approach, missing data methods such as full-information maximum likelihood or multiple imputation are not necessary, since we make no claims about larger populations that include both potential attritors and non-attritors. On the other hand, causal inference for non-attritors will be biased if attrition induces a selection effect–that is, if different students would attrit under different experimental conditions, and if these differences correlate with experimental outcomes. We assess the threat of attrition bias in two ways–first, by comparing attrition rates across conditions, and second by checking if non-attritting students assigned to different conditions were equivalent at baseline. More details can be found at WWC (2020).

## Calculating Attrition Rates

```{r wccAttritionNumbers}
Ncond=table(dat0$Z)
N=sum(Ncond)
ncond=table(dat$Z)
n=sum(ncond)

virtn=tapply(dat$virtual,dat$Z,sum)
virtN=tapply(dat0$virtual,dat0$Z,sum)

attCond=1-ncond/Ncond

diffAtt=outer(attCond,attCond,"-")
overallPair=diffAtt
for(cc in names(ncond))
  for(dd in names(ncond))
    overallPair[cc,dd]=1-(ncond[cc]+ncond[dd])/(Ncond[cc]+Ncond[dd])
```

```{r attritionTable}
tab=rbind(
  `# Randomized`=Ncond,
  `# in Analysis Sample`=ncond,
  `# Attrition Rate (%)`=attCond*100)
colnames(tab)=condNames[colnames(tab)]
tab=cbind(Overall=c(N,n,100*(1-n/N)),tab)
kable(tab,digits=1,caption="Table 1: Attrition rates by experimental condition")
```

To compare overall and differential attrition for pairwise comparisons of FH2T, DragonBox, and Immediate Feedback to Active Control, we calculate overall attrition as the number attrited for both conditions combined, divided by the number randomized; differential attrition is simply the difference of the numbers in the third row of Table 1 for the two conditions under comparison. The results are in Table 2.

```{r pairwiseAttritionTable}
tab=matrix(nrow=4,ncol=4)
tab[lower.tri(tab)]=overallPair[lower.tri(overallPair)]*100
tab[upper.tri(tab)]=diffAtt[upper.tri(diffAtt)]*100
rownames(tab)=colnames(tab)=condNames[colnames(diffAtt)]
kable(tab,digits=1,caption="Table 2: Overall attrition for pairwise comparisons in the lower triangular, and the differential attrition in the upper triangular.")
```

Figure 1 superimposes the results in Table 2, for Active control comparisons, onto Figure II.2 from the What Works Clearinghouse (WWC) Standards Handbook, Version 4.1 (WWC 2020), showing that attrition is tolerable under optimistic and/or cautious assumptions.

Figure 1: Attrition rates for Active control comparisons, compared to WWC standards.
```{r wwcPlot}
## plot
wwc <- read.csv("../attritionReports/wwc.csv")
names(wwc)[1] <- "Overall"

with(wwc,
  plot(Overall,Differential1,type="l",ylim=c(0,11),
       main="WWC Attrition Standards",
       xlab="Overall Attrition", ylab="Differential Attrition"))
  polygon(c(0,0,65,65),c(0,11,11,0),col="red")
  polygon(c(0,wwc[[1]],65),c(0,wwc$Differential1,0),col="yellow")
  polygon(c(0,wwc[[1]]),c(0,wwc$Differential0),col="green")

diffAtt=abs(diffAtt)

for(i in 1:3)
  for(j in (i+1):4){
    if('BAU'%in%colnames(diffAtt)[c(i,j)]){
      ov=overallPair[i,j]*100
      diff=diffAtt[i,j]*100
      points(ov,diff,pch=16)
    }
    # text(ov,diff,
    #      paste(condNames[i],'vs',condNames[j]),pos=2)
  }

##Position text by hand
# text(
#   overallPair['FH2T','Dragon']*100,
#   diffAtt['FH2T','Dragon']*100-0.1,
#   paste(condNames['FH2T'],'vs.',condNames['Dragon']),
#   pos=2)
# text(
#   overallPair['Dragon','if']*100,
#   diffAtt['if','Dragon']*100+0.15,
#   paste(condNames['if'],'vs.',condNames['Dragon']),
#   pos=2)

text(
  overallPair['Dragon','BAU']*100,
  diffAtt['Dragon','BAU']*100+0.1,
  paste(condNames['Dragon'],'vs.',condNames['BAU']),
  pos=2)
text(
  overallPair['ASSISTments','BAU']*100,
  diffAtt['ASSISTments','BAU']*100+0.15,
  paste(condNames['ASSISTments'],'vs.',condNames['BAU']),
  pos=2)
text(
  overallPair['FH2T','BAU']*100,
  diffAtt['FH2T','BAU']*100-0.15,
  paste(condNames['FH2T'],'vs.',condNames['BAU']),
  pos=2)

# text(
#   overallPair['FH2T','ASSISTments']*100,
#   diffAtt['FH2T','ASSISTments']*100+0.15,
#   paste(condNames['FH2T'],'vs.',condNames['ASSISTments']),
#   pos=2)
```

Finally, the following is a test of null hypothesis that the probability of attrition is equal across the four conditions:
```{r testingDifferentialAttrition}
#### testing differential attrition
prop.test(as.vector(Ncond-ncond),as.vector(Ncond))
```

### Attrition rates for students in Virtual or In-Person Conditions

Anecdotal evidence from teachers and other education professionals involved in the study suggested that some students assigned to the DragonBox condition who began the school year remotely had difficulties installing the DragonBox software, and that this may have led to higher attrition rates.
To see whether this may be so, we decompose attrition rates by virtual versus in-person status.

```{r virtAttrition}
attCond=1-virtn/virtN
N=sum(virtN)
n=sum(virtn)

tabVirt=rbind(
  `# Randomized`=virtN,
  `# in Analysis Sample`=virtn,
  `# Attrition Rate (%)`=attCond*100)
colnames(tabVirt)=condNames[colnames(tabVirt)]
tabVirt=cbind(Overall=c(N,n,100*(1-n/N)),tabVirt)

virtAtt=round((1-n/N)*100)

kable(tabVirt,digits=1,caption="Table 3: Population and sample sizes and attrition rates for virtual students")
```

```{r ipAttrition}
ipn=ncond-virtn
ipN=Ncond-virtN

attCond=1-ipn/ipN
N=sum(ipN)
n=sum(ipn)

ipAtt=round((1-n/N)*100)

tabIP=rbind(
  `# Randomized`=ipN,
  `# in Analysis Sample`=ipn,
  `# Attrition Rate (%)`=attCond*100)
colnames(tabIP)=condNames[colnames(tabIP)]
tabIP=cbind(Overall=c(N,n,100*(1-n/N)),tabIP)
## Baseline equivalence between conditions

kable(tabIP,digits=1,caption="Table 4: Population and sample sizes and attrition rates for in-person students")
```

```{r func}
da=function(TAB,cont)
  round(TAB[3,cont]-TAB[3,'Active Control'],1)
```

Overall attrition rates are substantially higher for virtual students (roughly `r virtAtt`\%) than for in-person students (roughly `r ipAtt`\%).
Differential attrition is similar for virtual and in-person students when comparing Active Control to FH2T (`r da(tabVirt,'FH2T')` vs `r da(tabIP,'FH2T')`) or to Immediate Feedback (`r da(tabVirt,'Immediate Feedback')` vs `r da(tabIP,'Immediate Feedback')`).
However, when comparing Active Control to DragonBox, differential attrition is nearly double for virtual students (`r da(tabVirt,'DragonBox')`) than for in-person students (`r da(tabIP,'DragonBox')`), providing some support for the hypothesis that virtual students assigned to DragonBox had particular difficulty participating in the experiment.

Notably, the overall and differential attrition rates for virtual students do not meet either the optimistic or cautious WWC standards, while the rates for in-person students meet both sets of standards.

## Baseline Equivalence for Non-Attritors

To further check whether differential attrition was informative or ignorable, we compared covariate means between non-attritting students assigned to each of the three experimental conditions (FH2T, DragonBox, and Immediate Feedback) and non-attritting students assigned to the Active Control.

Figure 2 plots covariate distributions across the four treatment groups; little imbalance is apparent.



```{r balancePlot, fig.cap="Figure 2: Comparing covariates across conditions"}
### balance plot:
dat%>%
    filter(hasPosttest)%>%
    mutate(race=substr(as.character(race),1,1))%>%
  balPlot(covNames,trtVar="Z",data=.)
```

### Estimating Baseline Equivalence

First, we estimated covariate imbalance between conditions for each covariate, in the scale recommended by WWC (2020).
In particular, for pretest (a numeric variable), we estimated Hedges' g comparing pretest between conditions "A" and "B" as:
$$ g=\left(1-\frac{3}{4(n_A+n_B)-9}\right)\frac{\bar{x}_A-\bar{x}_B}{\sqrt{\frac{(n_A-1)s_A^2+(n_B-1)s_B^2}{n_A+n+B-2}}}$$
where $\bar{x}_A$, $S^2_A$, and $n_A$ are the average and sample variance pretest and sample size in condition A, and $\bar{x}_B$, $S^2_B$, and $n_B$ are defined similarly.

For binary or categorical variables, we estimated the Cox index. Let $p_A$ and $p_B$ be the sample proportions in conditions A and B. Then
$$ d_{Cox}=\frac{1}{1.65}log\left(\frac{p_A(1-p_B)}{p_B(1-p_A)}\right)$$
See WWC (2020b), p. 15.

The results for the three Active control comparisons are in Table 5 and Figure 3. All of the differences are below the upper WWC threshold of 0.25, though several are above the lower threshold of 0.05, indicating the need for statistical adjustment.

```{r effectSizes}
##### standardized differences following WWC procedures 4.1 chp VI p. 15
dCox=function(x,z){
  pi=mean(x[z],na.rm=TRUE)
  pc=mean(x[!z],na.rm=TRUE)
  (qlogis(pi)-qlogis(pc))/1.65
}

hedgesG=function(x,z){
  yi=mean(x[z],na.rm=TRUE)
  yc=mean(x[!z],na.rm=TRUE)
  ni=sum(!is.na(x[z]))
  nc=sum(!is.na(x[!z]))
  N=ni+nc
  omega=1-3/(4*N-9)
  omega*(yi-yc)/sqrt(((ni-1)*var(x[z])+(nc-1)*var(x[!z]))/(N-2))
}

stdDiffs=list()
for(cond in levels(dat$Z)[-1]){
  diffDat=
    dat%>%
    filter(Z%in%c(cond,'BAU'))%>%
    mutate(Z=Z==cond)%>%
    select(all_of(covNames),Z)
  diffs=map_dbl(setdiff(covNames,c('pretest','race'))%>%setNames(.,.),
                ~dCox(diffDat[[.]],diffDat$Z))
  for(r in levels(diffDat$race)[-1]){
    diffs=c(diffs,dCox(diffDat$race==r,diffDat$Z))
    names(diffs)[length(diffs)] <- r
  }
  diffs=c(diffs,pretest=hedgesG(diffDat$pretest,diffDat$Z))
  stdDiffs[[cond]] <- diffs
}
diffs <- as.data.frame(stdDiffs)

lapply(diffs,
       function(x)
         paste0(round(x,3),
                ifelse(abs(x)>0.05,'*','')))%>%
  as.data.frame(row.names=rownames(diffs))%>%
  kable(caption="Table 5: Cox indices (for binary or categorical covariates) and Hedges's g (for numeric covariates) comparing baseline covariate means between each of the experimental conditions and the Active Control (positive differences indicate higher means for the experimental conditions, compared to Active Control). Stars indicate effect sizes >0.05, for which WWC recommends statistical adjustment",
        digits=3)
```


```{r lovePlot, fig.cap="Figure 3: Effect Sizes comparing covariate means between each experimental condition and the Active Control. The solid line at 0 indicates identical means; effect sizes outside of the dashed lines at -0.05 and 0.05 indicate a need for statistical adjustment"}
diffs%>%rownames_to_column('covr')%>%pivot_longer(-covr,names_to = "Contrast",values_to = "Effect Size")%>%mutate(covr=ifelse(covr%in%c('Asian','Other'),paste('race:',covr),covr))%>%ggplot(aes(`Effect Size`,covr,color=Contrast))+geom_point()+geom_vline(xintercept=0)+geom_vline(xintercept=c(-0.05,0.05),linetype='dashed')+ylab(NULL)
```


### Testing the Null Hypothesis of Baseline Equivalence

If attrition is ignorable, then the observed treatment groups in the analysis sample--i.e. non-attritor--should be comparable.
Evidence against ignorable attrition could be expressed as evidence against the null hypothesis
$$H_0: X \perp Z$$
where $X$ is a covariate and $Z$ indicates treatment assignment.
In practice, we test the following simpler null hypothesis (which is also, arguably, more to the point):
$$H_0: \mathbf{E}[X|Z=A]=\mathbf{E}[X|Z=B]$$
of equality of expectation between treatment conditions A and B.

There are a number of methodological hurdles to a proper test.
First of all, the tests should account for the randomization blocking. Second, we conduct a test of $H_0$ for each covariate for three different Active control comparisons, leading to multiple comparisons and (potentially) inflated Type-I error rates.

We rely on methods described in Hansen and Bowers (2008) and implemented in the `RItools` package in `R`.
This includes a separate omnibus test for balance in all covariates for each of the comparisons.
It also accounts for blocking (for simplicity, we use classrooms as blocks) and computes tests for individual covariates.

Table 6 gives the test statistics and p-values for omnibus tests across all covariates:
```{r ritools}
covForm <- as.formula(paste("~",paste(covNames,collapse="+")))
totForm <- update(covForm,Z~.)
bals <- xbalMult(totForm,dat,trtLevs=unique(dat$Z),strata=list(noBlocks=NULL,cls=~class),na.rm=TRUE)

overall <- map_dfr(grep('BAU',names(bals),value=TRUE),~c(contrast=.,bals[[.]]$overall['cls',]))
nnn=gsub(' |vs\\.|BAU','',overall$contrast)
overall$contrast=c(ASSISTments='Immediate Feedback',Dragon='DragonBox',FH2T="FH2T")[nnn]

kable(overall,caption='Table 6: Overall covariate balance for each of the Active Control comparisons',digits=3)
```

There is some evidence for imbalance between the Active Control and DragonBox condition, although if we were to correct for having tested three separate null hypotheses, that result would cease to be statistically significant.
There is no evidence for imbalance in the other two contrasts.

Table 7 gives hypothesis test results (Z-scores) for each covariate in each Active Control comparison. across the three comparisons, only pretest is significantly imbalanced.
Since pretest is the most important covariate, imbalance in pretest across the groups is notable, even if multiplicity corrections would increase the p-values above 0.05.

```{r individualBalance}
ind <- map(bals[grep('BAU',names(bals),value=TRUE)],~data.frame(.$results[,c('z','p'),'cls']))
ind <- map(ind,~data.frame(Z=paste0(-round(.$z,3),ifelse(.$p<0.001,'***',ifelse(.$p<0.01,'**',ifelse(.$p<0.05,'*',ifelse(.$p<0.1,'.',''))))),row.names = rownames(.)))
nnn=gsub(' |vs\\.|BAU','',names(ind))
ind=as.data.frame(ind,row.names = rownames(ind[[1]]))
names(ind)=c(ASSISTments='Immediate Feedback',Dragon='DragonBox',FH2T="FH2T")[nnn]
kable(ind,caption='Table 7: Z-scores testing baseline covariate balance between each experimental condition and the Active Control')
```



```{r,eval=FALSE,include=FALSE}
tests <- map_dfr(covNames,~balTestOne(dat[[.]],dat$Z,dat$class))
tests <- tests[,sapply(tests,function(x) all(!is.na(x)))]
tests$p.adj <- p.adjust(tests$p.value,method="fdr")
tests$covariate <- covNames
tests%>%
  select(covariate,method,statistic,p.value,p.adj)%>%
  knitr::kable()




```
